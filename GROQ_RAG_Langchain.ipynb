{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rypotter/depo/blob/master/GROQ_RAG_Langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eredet author: https://www.linkedin.com/in/mickymultani/\n",
        "\n",
        "Eredeti a LinkedIn-en:\n",
        "https://www.linkedin.com/pulse/build-lightning-fast-rag-chatbot-powered-groqs-lpu-ollama-multani-ssloc/\n",
        "\n",
        "Eredeti Colab: https://colab.research.google.com/drive/1Obrby8RniFfjUvf3DhbNHC6-CmBdiXbY?usp=sharing\n",
        "\n",
        "Eredeti Github: https://github.com/mickymultani/Groq-RAG"
      ],
      "metadata": {
        "id": "8bMjrCjHDHuV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install groq langchain langchain-core langchain-groq chromadb pypdf gradio"
      ],
      "metadata": {
        "id": "zlKVdjMh_XM7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lshw"
      ],
      "metadata": {
        "id": "PNiGj7Y7YEGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://ollama.ai/install.sh | sh"
      ],
      "metadata": {
        "id": "iS5z9MDT_ygd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5QMCoJ0dHioq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After installing Ollama per the above cell, open the notebook terminal from the bottom left and start ollama by entering `ollama serve`. The following cells with ollama commands ***will only work*** if ollama is running!\n",
        "\n",
        "Az Ollama fenti cella szerinti telepítése után nyissa meg a notebook terminált a bal alsó sarokban, és indítsa el az ollama-t az ***ollama serve*** beírásával. A következő ollama parancsokat tartalmazó cellák csak akkor működnek, ha az ollama fut!\n",
        "\n",
        "A notebook terminál csak Colab Pro-ban müxik, ezért itt a ***shellinabox*** terminált használom. Vagy az iframe-ben kell elindítani a terminált, vagy böngésző lapon, és ott beírni a parancsot:  ***ollama serve***\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ua72mxG2Hi6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup shellinabox\n",
        "!apt install shellinabox &> /dev/null\n",
        "!nohup shellinaboxd --disable-ssl --no-beep --port=8000 --css /etc/shellinabox/options-enabled/00_White\\ On\\ Black.css -s \"/:root:root:/root:/bin/bash -c bash -i\" &> /dev/null &\n",
        "!yes | /usr/local/sbin/unminimize &> /dev/null"
      ],
      "metadata": {
        "id": "NCFTcOplHHp1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vagy\n",
        "# Open a terminal in an iframe\n",
        "from google.colab.output import serve_kernel_port_as_iframe\n",
        "serve_kernel_port_as_iframe(8000)"
      ],
      "metadata": {
        "id": "u9jidWGkHH7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vagy\n",
        "# Open a terminal in a browser tab\n",
        "from google.colab.output import serve_kernel_port_as_window\n",
        "serve_kernel_port_as_window(8000, anchor_text = \"Open a terminal\")"
      ],
      "metadata": {
        "id": "HadgsixgISsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull nomic-embed-text"
      ],
      "metadata": {
        "id": "W07TkVrPD8g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ri6ydX5e_R3L"
      },
      "outputs": [],
      "source": [
        "from langchain_groq import ChatGroq\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import OllamaEmbeddings\n",
        "from langchain_community import embeddings\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain_community.chat_models import ChatOllama\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import time\n",
        "import textwrap\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFDirectoryLoader(\"data\")\n",
        "the_text = loader.load()\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = text_splitter.split_documents(the_text)"
      ],
      "metadata": {
        "id": "18lJKz90fDX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For text files\n",
        "# file_path = r'data/YourTextFileName.txt'\n",
        "# loader = TextLoader(file_path)\n",
        "# the_text = loader.load()\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
        "# chunks = text_splitter.split_documents(the_text)"
      ],
      "metadata": {
        "id": "xf_4hDUREuq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore = Chroma.from_documents(\n",
        "    documents=chunks,\n",
        "    collection_name=\"ollama_embeds\",\n",
        "    embedding=embeddings.ollama.OllamaEmbeddings(model='nomic-embed-text'),\n",
        ")\n",
        "retriever = vectorstore.as_retriever()"
      ],
      "metadata": {
        "id": "izMS2O_TE0mB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%env GROQ_API_KEY=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\n",
        "#api_key=os.environ.get(\"GROQ_API_KEY\")\n",
        "api_key=os.environ[\"GROQ_API_KEY\"]\n",
        "groq_api_key = api_key # userdata.get('groq_api_key')\n",
        "print(groq_api_key)"
      ],
      "metadata": {
        "id": "VJf_QWGgLG2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatGroq(\n",
        "            groq_api_key=groq_api_key,\n",
        "            model_name='mixtral-8x7b-32768'\n",
        "    )"
      ],
      "metadata": {
        "id": "ZRwArSOaACQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_template = \"\"\"Answer the question based only on the following context:\n",
        "{context}\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "rag_prompt = ChatPromptTemplate.from_template(rag_template)\n",
        "rag_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
        "    | rag_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")"
      ],
      "metadata": {
        "id": "sfN4QX1XFVjP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the architecture with a simple hard coded question\n",
        "response = rag_chain.invoke(\"What is this document about\")\n",
        "print(textwrap.fill(response, width=80))"
      ],
      "metadata": {
        "id": "jDqTM5tMm4dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make the questions dynamic using a chat interface. Let's use gradio for this.\n",
        "def process_question(user_question):\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Directly using the user's question as input for rag_chain.invoke\n",
        "    response = rag_chain.invoke(user_question)\n",
        "\n",
        "    # Measure the response time\n",
        "    end_time = time.time()\n",
        "    response_time = f\"Response time: {end_time - start_time:.2f} seconds.\"\n",
        "\n",
        "    # Combine the response and the response time into a single string\n",
        "    full_response = f\"{response}\\n\\n{response_time}\"\n",
        "\n",
        "    return full_response\n",
        "\n",
        "# Setup the Gradio interface\n",
        "iface = gr.Interface(fn=process_question,\n",
        "                     inputs=gr.Textbox(lines=2, placeholder=\"Type your question here...\"),\n",
        "                     outputs=gr.Textbox(),\n",
        "                     title=\"GROQ CHAT\",\n",
        "                     description=\"Ask any question about your document, and get an answer along with the response time.\")\n",
        "\n",
        "# Launch the interface\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "x5SfJrZjlPOL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}