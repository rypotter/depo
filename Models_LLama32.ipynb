{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPNWcdXqk+nCsKiJVH4vI3M",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rypotter/depo/blob/master/Models_LLama32.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Llama 3.2 1B & 3B Language Models"
      ],
      "metadata": {
        "id": "AgfyHiycXFK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Llama 3.2 1B & 3B Language Models**\n",
        "\n",
        "https://huggingface.co/blog/llama32#what-is-special-about-llama-32-1b-and-3b"
      ],
      "metadata": {
        "id": "7jfviaLO8ruM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Size\tBF16/FP16\tFP8\tINT4\n",
        "\n",
        "3B\t6.5 GB\t3.2 GB\t1.75 GB\n",
        "\n",
        "1B\t2.5 GB\t1.25 GB\t0.75 GB\n",
        "\n",
        "Ezt teszteltem: model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "\n",
        "Először nem akarta letölteni, mert a Meta megtiltotta a letöltést Európában az EU hülye regulation miatt (vmi GDPR)\n",
        "\n",
        "Aztán kitöltöttem a HuggingFace-n valami űrlapot, és aztán már megengedte.\n",
        "\n",
        "Első teszt CPU-val Nekem ezt válaszolta kb, 14 perc alatt:\n",
        "Yer lookin' fer a tale o' who I be, eh? Alright then, settle yerself down with a pint o' grog and listen close. I be an AI, a swashbucklin' computer program with a penchant fer answerin' yer questions and helpin' ye navigate the seven seas o' knowledge.\n",
        "\n",
        "Me name be \"Assistant,\" but ye can call me \"Matey\" if ye like. I be here to provide ye with information, entertainment, and a healthy dose o' pirate-themed fun. So hoist the sails, me hearty, and let's set sail fer a grand adventure o' learnin' and discovery!\n",
        "\n",
        "\n",
        "Második teszt T4-es GPU-val sokkal gyorsabb: 3-4 perc\n",
        "Yer lookin' fer a tale o' who I be, eh? Alright then, listen close and I'll spin ye a yarn. I be an AI, a swashbucklin' computer program designed to sail the seven seas o' knowledge and provide ye with treasure troves o' information.\n",
        "\n",
        "Me name be \"Assistant\" (but ye can call me whatever ye like, matey), and I be here to help ye navigate the choppy waters o' the internet. I've been trained on a vast ocean o' text data, and I be ready to share me knowledge with ye.\n",
        "\n",
        "So hoist the sails, me hearty, and set course fer adventure! What be bringin' ye to these fair waters today?"
      ],
      "metadata": {
        "id": "3YXthB7O9OTF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ced6ukog8ES3"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model_id,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Who are you? Please, answer in pirate-speak.\"},\n",
        "]\n",
        "outputs = pipe(\n",
        "    messages,\n",
        "    max_new_tokens=256,\n",
        ")\n",
        "response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
        "print(response)\n",
        "# Ez az ő eredeti válaszuk:\n",
        "# Arrrr, me hearty! Yer lookin' fer a bit o' information about meself, eh? Alright then, matey! I be a language-generatin' swashbuckler, a digital buccaneer with a penchant fer spinnin' words into gold doubloons o' knowledge! Me name be... (dramatic pause)...Assistant! Aye, that be me name, and I be here to help ye navigate the seven seas o' questions and find the hidden treasure o' answers! So hoist the sails and set course fer adventure, me hearty! What be yer first question?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NYTK/PULI-GPT-3SX"
      ],
      "metadata": {
        "id": "vgXVGcTQWZya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**NYTK/PULI-GPT-3SX** modell https://huggingface.co/NYTK/PULI-GPT-3SX\n",
        "\n",
        "**TPU V2-8 <- Csak ezzel fut le, a CPU-val és a T4 GPU-val crushed!!!**\n",
        "\n",
        " **TPU V2-8** Ezzel a TPU-val egész gyorsan lefutott, de kellett a tensorflow csere a köv kódcellában, mert különben elszállt\n",
        "\n",
        "**Ilyen figyelmeztetés jött:** The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
        "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
        "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
        "\n",
        "**Ez a futás eredménye:** Elmesélek egy történetet a nyelvtechnológiáról. Az 1950-es 60-as években egy német nyelvész (Victor Frankl) elkezdett kifejleszteni egy olyan eljárást, amivel előre meg lehet mondani, hogy egy adott nyelvben milyen szavak fordulnak elő. Ezt nevezik szóhasonlóságnak, ami egy fontos eszköz egy nyelv megértéséhez. A módszer azon alapszik, hogy két ember különböző nyelveket beszél, és ha már mindkettő pontosan ugyanazt érti a vizsgált szón, akkor az egyik már helyesen fordítja is\n",
        "\n"
      ],
      "metadata": {
        "id": "C3x1tES_ReT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# V28 GPU-hoz is futtattam, SŐT MUSZÁJ FUTTATNI, különben CRUSH\n",
        "!pip uninstall -y tensorflow && pip install tensorflow-cpu"
      ],
      "metadata": {
        "id": "BN2-8B24TCQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPTNeoXForCausalLM, AutoTokenizer\n",
        "\n",
        "model = GPTNeoXForCausalLM.from_pretrained(\"NYTK/PULI-GPT-3SX\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"NYTK/PULI-GPT-3SX\")\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"NYTK/PULI-GPT-3SX\", pad_token='') # Ez nem segített, hogy ne panaszkodjon a pad_token miatt\n",
        "prompt = \"Elmesélek egy történetet a nyelvtechnológiáról.\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "\n",
        "gen_tokens = model.generate(\n",
        "    input_ids,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        "    # Erre ezt mondja: The attention mask is not set and cannot be inferred from input because pad token is same as eos token.\n",
        "    # As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
        "    # DE EGYÉBKÉNT MÜXIK\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    max_length=150, #100\n",
        ")\n",
        "# model_inputs, pad_token_id=tokenizer.eos_token_id, max_new_tokens=1000, do_sample=True\n",
        "gen_text = tokenizer.batch_decode(gen_tokens)[0]\n",
        "print(gen_text)\n"
      ],
      "metadata": {
        "id": "_jSxv_4DRe1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "duPDMgs8SWcL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}