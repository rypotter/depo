{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rypotter/depo/blob/master/OpenAI_Assistant_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pradip Nichite https://github.com/PradipNichite/Youtube-Tutorials/tree/main\n",
        "\n",
        "https://www.youtube.com/watch?v=yo0qy7xyd3A\n",
        "\n",
        "Download as a html: https://www.futuresmart.ai/"
      ],
      "metadata": {
        "id": "st_YZ7voZhMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "G9k10WZJI41t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3115e28d-d3da-4955-d365-966870f97b3b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-1.3.4-py3-none-any.whl (220 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/220.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.9/220.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m220.5/220.5 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<4,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.25.1-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.0/75.0 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from openai) (1.10.13)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai) (4.66.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.5 in /usr/local/lib/python3.10/dist-packages (from openai) (4.5.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<4,>=3.5.0->openai) (1.1.3)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai) (2023.7.22)\n",
            "Collecting httpcore (from httpx<1,>=0.23.0->openai)\n",
            "  Downloading httpcore-1.0.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore->httpx<1,>=0.23.0->openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: h11, httpcore, httpx, openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed h11-0.14.0 httpcore-1.0.2 httpx-0.25.1 openai-1.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "client = OpenAI(api_key=\"key\")\n",
        "client"
      ],
      "metadata": {
        "id": "HA7glsvHJdop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08d2ba1c-67b1-4e06-e1a7-316055a89d71"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<openai.OpenAI at 0x7a413f57d150>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Upload File"
      ],
      "metadata": {
        "id": "jSO3_datOyJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_file = client.files.create(\n",
        "    file=open(\"/content/FutureSmart AI_ Custom Natural Language Processing Solutions _ NLP _ GPT-3 _ ChatGPT _ Semantic Search..html\", 'rb'),\n",
        "    purpose='assistants',\n",
        ")"
      ],
      "metadata": {
        "id": "DE6f7qaZKWZW",
        "outputId": "4b631ebc-a662-4134-8103-46079bb754e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-17839703bf92>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m uploaded_file = client.files.create(\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/FutureSmart AI_ Custom Natural Language Processing Solutions _ NLP _ GPT-3 _ ChatGPT _ Semantic Search..html\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mpurpose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'assistants'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m )\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/FutureSmart AI_ Custom Natural Language Processing Solutions _ NLP _ GPT-3 _ ChatGPT _ Semantic Search..html'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnHtMZI7aw8B",
        "outputId": "bf884348-26c5-4d42-8f8e-e10993596f34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FileObject(id='file-EWKqY7PCZ6KMDjnLV6hR6pcP', bytes=464362, created_at=1699689980, filename='FutureSmart AI_ Custom Natural Language Processing Solutions _ NLP _ GPT-3 _ ChatGPT _ Semantic Search..html', object='file', purpose='assistants', status='processed', status_details=None)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Assistant"
      ],
      "metadata": {
        "id": "-gFb8p-MO6Cm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9CpYWwGIxMK"
      },
      "outputs": [],
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=\"Futuresmart AI Assistent\",\n",
        "    instructions=\"You are a AI Assistent that answers any queries related to Futuresmart AI\",\n",
        "    tools=[{\"type\": \"retrieval\"}],\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    file_ids=[uploaded_file.id]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Thread"
      ],
      "metadata": {
        "id": "h91zPAXAO-yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thread = client.beta.threads.create()"
      ],
      "metadata": {
        "id": "aFBhcp8AI6tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTkVBF44b33K",
        "outputId": "25ec0c12-125e-4386-aa13-3efb596cd3d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Thread(id='thread_B9IWzBfHFD2AQaCNf0cd3pmh', created_at=1699690277, metadata={}, object='thread')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what is Futuresmart AI?\n",
        "# What services does it offer?\n",
        "# What tech stack does Futuresmart AI use?\n",
        "# What clients says about Futuresmart AI?"
      ],
      "metadata": {
        "id": "ihK8N3hSM70W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Message Inside Thread"
      ],
      "metadata": {
        "id": "wM0X1ZRUPEcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"What clients says about Futuresmart AI?\"\n",
        ")"
      ],
      "metadata": {
        "id": "aQCeqHU3LLyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.beta.threads.messages.list(thread_id=thread.id).data\n",
        "# client.beta.threads.messages.list(thread_id=thread.id).data[0].content[0].text.value"
      ],
      "metadata": {
        "id": "cctlqZDBMLwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d35b35-b7d1-4c5f-cf64-50400d607a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ThreadMessage(id='msg_ewzcNNhodLaLLirBC7nmmgjN', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='What services does it offer?'), type='text')], created_at=1699690636, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_B9IWzBfHFD2AQaCNf0cd3pmh'),\n",
              " ThreadMessage(id='msg_1jMoPyojABVNYWoUtagDH8mK', assistant_id='asst_qaBEk8siaY1mdOcrHAOj8UpB', content=[MessageContentText(text=Text(annotations=[TextAnnotationFileCitation(end_index=324, file_citation=TextAnnotationFileCitationFileCitation(file_id='file-EWKqY7PCZ6KMDjnLV6hR6pcP', quote='FutureSmart AI provides custom Natural Language Processing (NLP) solutions for companies looking to get ahead of the future.\\\\nOur dedicated team of Data Scientists and ML Engineers provides an end-to-end solution from data labeling to modeling and deploying an ML model tailored to your specific use case'), start_index=314, text='【9†source】', type='file_citation')], value=\"FutureSmart AI is a company that provides custom Natural Language Processing (NLP) solutions tailored for companies seeking to advance their capabilities with AI. They offer an end-to-end solution encompassing data labeling, modeling, and deploying machine learning models specific to clients' individual use cases【9†source】.\"), type='text')], created_at=1699690436, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_HJLeqkxODRz5aIaXBhwSsiTz', thread_id='thread_B9IWzBfHFD2AQaCNf0cd3pmh'),\n",
              " ThreadMessage(id='msg_hhqFIj7r6zC6CP91TKaq2geT', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='what is Futuresmart AI?'), type='text')], created_at=1699690354, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_B9IWzBfHFD2AQaCNf0cd3pmh')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run Assistent"
      ],
      "metadata": {
        "id": "UuVSLBSJPJpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id\n",
        ")\n",
        "# run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
        "# run"
      ],
      "metadata": {
        "id": "wEpXo8P8LTJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Retrive Assistant's Response"
      ],
      "metadata": {
        "id": "uij6eRS9PXc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
        "    if run.status==\"completed\":\n",
        "        messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "        latest_message = messages.data[0]\n",
        "        text = latest_message.content[0].text.value\n",
        "        print(text)\n",
        "        break;"
      ],
      "metadata": {
        "id": "WbQuRT7QLb7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95430336-6bcb-4358-84bb-bee36354b0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clients have provided positive testimonials about their experiences with FutureSmart AI:\n",
            "\n",
            "1. One client praised the engineer as very hardworking and talented, with high integrity in their work and transparency in work progress. They highly recommended them for work related to Natural Language Processing (NLP) in the AI space【32†source】.\n",
            "\n",
            "2. Another client highlighted the expertise in NLP and MLOps and appreciated that the engineer not only completed the job but also proposed best practices and further approaches that added value to the final product. They remarked on the smooth communication and quick comprehension skills, expressing that the collaboration would continue for a long time【33†source】.\n",
            "\n",
            "3. The same sentiments were echoed by a client named Binoocle from Upwork, indicating a strong ongoing partnership【34†source】.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Ohya4cwNfBu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}