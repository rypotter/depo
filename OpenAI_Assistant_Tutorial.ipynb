{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rypotter/depo/blob/master/OpenAI_Assistant_Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pradip Nichite https://github.com/PradipNichite/Youtube-Tutorials/tree/main\n",
        "\n",
        "https://www.youtube.com/watch?v=yo0qy7xyd3A\n",
        "\n",
        "Download as a html: https://www.futuresmart.ai/"
      ],
      "metadata": {
        "id": "st_YZ7voZhMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade openai"
      ],
      "metadata": {
        "id": "G9k10WZJI41t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "# openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "client = OpenAI(api_key=\"key\")\n",
        "client"
      ],
      "metadata": {
        "id": "HA7glsvHJdop",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8ed6b41-cb03-420f-ab33-b2c978e637b6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<openai.OpenAI at 0x7e9283eed990>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://github.com/rypotter/depo/tree/master/content/FutureSmart AI_ Custom Natural Language Processing Solutions _ NLP _ GPT-3 _ ChatGPT _ Semantic Search..html\"\n"
      ],
      "metadata": {
        "id": "kpmvPGPMzfIv",
        "outputId": "57041c2a-7199-44ce-ab4e-028748cd3d16",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-24 10:13:36--  https://github.com/rypotter/depo/tree/master/content/FutureSmart%20AI_%20Custom%20Natural%20Language%20Processing%20Solutions%20_%20NLP%20_%20GPT-3%20_%20ChatGPT%20_%20Semantic%20Search..html\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://github.com/rypotter/depo/blob/master/content/FutureSmart%20AI_%20Custom%20Natural%20Language%20Processing%20Solutions%20_%20NLP%20_%20GPT-3%20_%20ChatGPT%20_%20Semantic%20Search..html [following]\n",
            "--2023-11-24 10:13:36--  https://github.com/rypotter/depo/blob/master/content/FutureSmart%20AI_%20Custom%20Natural%20Language%20Processing%20Solutions%20_%20NLP%20_%20GPT-3%20_%20ChatGPT%20_%20Semantic%20Search..html\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1060671 (1.0M) [text/plain]\n",
            "Saving to: ‘FutureSmart AI_ Custom Natural Language Processing Solutions _ NLP _ GPT-3 _ ChatGPT _ Semantic Search..html.1’\n",
            "\n",
            "FutureSmart AI_ Cus 100%[===================>]   1.01M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-11-24 10:13:36 (18.1 MB/s) - ‘FutureSmart AI_ Custom Natural Language Processing Solutions _ NLP _ GPT-3 _ ChatGPT _ Semantic Search..html.1’ saved [1060671/1060671]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Upload File"
      ],
      "metadata": {
        "id": "jSO3_datOyJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_file = client.files.create(\n",
        "    file=open(\"/content/FutureSmart AI_ Custom Natural Language Processing Solutions _ NLP _ GPT-3 _ ChatGPT _ Semantic Search..html\", 'rb'),\n",
        "    purpose='assistants',\n",
        ")"
      ],
      "metadata": {
        "id": "DE6f7qaZKWZW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uploaded_file"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnHtMZI7aw8B",
        "outputId": "b5da50e8-901e-45c1-ffdf-73f964b351b4"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FileObject(id='file-IRIdt49ufj5UGM0cdNPRwHO1', bytes=1060671, created_at=1700820842, filename='FutureSmart AI_ Custom Natural Language Processing Solutions _ NLP _ GPT-3 _ ChatGPT _ Semantic Search..html', object='file', purpose='assistants', status='processed', status_details=None)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Assistant"
      ],
      "metadata": {
        "id": "-gFb8p-MO6Cm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "c9CpYWwGIxMK",
        "outputId": "4add1229-519c-4f51-b17c-fc0e4073722e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a8c034100525>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m assistant = client.beta.assistants.create(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Futuresmart AI Assistent\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0minstructions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"You are a AI Assistent that answers any queries related to Futuresmart AI\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtools\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"retrieval\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4-1106-preview\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/beta/assistants/assistants.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, model, description, file_ids, instructions, metadata, name, tools, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m     93\u001b[0m         \"\"\"\n\u001b[1;32m     94\u001b[0m         \u001b[0mextra_headers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"OpenAI-Beta\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"assistants=v1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mextra_headers\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0;34m\"/assistants\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1061\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m         )\n\u001b[0;32m-> 1063\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    840\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    841\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 842\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    843\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    844\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    883\u001b[0m             \u001b[0;31m# to completion before attempting to access the response text.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m             \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mretries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"The requested model 'gpt-4-1106-preview' does not exist.\", 'type': 'invalid_request_error', 'param': 'model', 'code': 'model_not_found'}}"
          ]
        }
      ],
      "source": [
        "assistant = client.beta.assistants.create(\n",
        "    name=\"Futuresmart AI Assistent\",\n",
        "    instructions=\"You are a AI Assistent that answers any queries related to Futuresmart AI\",\n",
        "    tools=[{\"type\": \"retrieval\"}],\n",
        "    model=\"gpt-4-1106-preview\",\n",
        "    file_ids=[uploaded_file.id]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Thread"
      ],
      "metadata": {
        "id": "h91zPAXAO-yj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thread = client.beta.threads.create()"
      ],
      "metadata": {
        "id": "aFBhcp8AI6tW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "thread"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTkVBF44b33K",
        "outputId": "25ec0c12-125e-4386-aa13-3efb596cd3d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Thread(id='thread_B9IWzBfHFD2AQaCNf0cd3pmh', created_at=1699690277, metadata={}, object='thread')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# what is Futuresmart AI?\n",
        "# What services does it offer?\n",
        "# What tech stack does Futuresmart AI use?\n",
        "# What clients says about Futuresmart AI?"
      ],
      "metadata": {
        "id": "ihK8N3hSM70W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create Message Inside Thread"
      ],
      "metadata": {
        "id": "wM0X1ZRUPEcu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "message = client.beta.threads.messages.create(\n",
        "    thread_id=thread.id,\n",
        "    role=\"user\",\n",
        "    content=\"What clients says about Futuresmart AI?\"\n",
        ")"
      ],
      "metadata": {
        "id": "aQCeqHU3LLyi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client.beta.threads.messages.list(thread_id=thread.id).data\n",
        "# client.beta.threads.messages.list(thread_id=thread.id).data[0].content[0].text.value"
      ],
      "metadata": {
        "id": "cctlqZDBMLwz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46d35b35-b7d1-4c5f-cf64-50400d607a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[ThreadMessage(id='msg_ewzcNNhodLaLLirBC7nmmgjN', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='What services does it offer?'), type='text')], created_at=1699690636, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_B9IWzBfHFD2AQaCNf0cd3pmh'),\n",
              " ThreadMessage(id='msg_1jMoPyojABVNYWoUtagDH8mK', assistant_id='asst_qaBEk8siaY1mdOcrHAOj8UpB', content=[MessageContentText(text=Text(annotations=[TextAnnotationFileCitation(end_index=324, file_citation=TextAnnotationFileCitationFileCitation(file_id='file-EWKqY7PCZ6KMDjnLV6hR6pcP', quote='FutureSmart AI provides custom Natural Language Processing (NLP) solutions for companies looking to get ahead of the future.\\\\nOur dedicated team of Data Scientists and ML Engineers provides an end-to-end solution from data labeling to modeling and deploying an ML model tailored to your specific use case'), start_index=314, text='【9†source】', type='file_citation')], value=\"FutureSmart AI is a company that provides custom Natural Language Processing (NLP) solutions tailored for companies seeking to advance their capabilities with AI. They offer an end-to-end solution encompassing data labeling, modeling, and deploying machine learning models specific to clients' individual use cases【9†source】.\"), type='text')], created_at=1699690436, file_ids=[], metadata={}, object='thread.message', role='assistant', run_id='run_HJLeqkxODRz5aIaXBhwSsiTz', thread_id='thread_B9IWzBfHFD2AQaCNf0cd3pmh'),\n",
              " ThreadMessage(id='msg_hhqFIj7r6zC6CP91TKaq2geT', assistant_id=None, content=[MessageContentText(text=Text(annotations=[], value='what is Futuresmart AI?'), type='text')], created_at=1699690354, file_ids=[], metadata={}, object='thread.message', role='user', run_id=None, thread_id='thread_B9IWzBfHFD2AQaCNf0cd3pmh')]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Run Assistent"
      ],
      "metadata": {
        "id": "UuVSLBSJPJpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run = client.beta.threads.runs.create(\n",
        "  thread_id=thread.id,\n",
        "  assistant_id=assistant.id\n",
        ")\n",
        "# run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
        "# run"
      ],
      "metadata": {
        "id": "wEpXo8P8LTJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Retrive Assistant's Response"
      ],
      "metadata": {
        "id": "uij6eRS9PXc9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    run = client.beta.threads.runs.retrieve(thread_id=thread.id, run_id=run.id)\n",
        "    if run.status==\"completed\":\n",
        "        messages = client.beta.threads.messages.list(thread_id=thread.id)\n",
        "        latest_message = messages.data[0]\n",
        "        text = latest_message.content[0].text.value\n",
        "        print(text)\n",
        "        break;"
      ],
      "metadata": {
        "id": "WbQuRT7QLb7R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95430336-6bcb-4358-84bb-bee36354b0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clients have provided positive testimonials about their experiences with FutureSmart AI:\n",
            "\n",
            "1. One client praised the engineer as very hardworking and talented, with high integrity in their work and transparency in work progress. They highly recommended them for work related to Natural Language Processing (NLP) in the AI space【32†source】.\n",
            "\n",
            "2. Another client highlighted the expertise in NLP and MLOps and appreciated that the engineer not only completed the job but also proposed best practices and further approaches that added value to the final product. They remarked on the smooth communication and quick comprehension skills, expressing that the collaboration would continue for a long time【33†source】.\n",
            "\n",
            "3. The same sentiments were echoed by a client named Binoocle from Upwork, indicating a strong ongoing partnership【34†source】.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Ohya4cwNfBu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}